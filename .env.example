# LVECDE Environment Configuration
# Copy this file to .env and configure for your setup

# ============================================================================
# Local AI Configuration
# ============================================================================

# Local AI Server URL
# Examples:
#   - Ollama: http://localhost:11434
#   - LM Studio: http://localhost:1234/v1
#   - LocalAI: http://localhost:8080/v1
#   - Remote server: http://192.168.1.100:11434
EXPO_PUBLIC_LOCAL_AI_URL=http://localhost:11434

# Model to use
# Examples:
#   - Ollama: codellama:13b, deepseek-coder:6.7b, wizardcoder:15b
#   - LM Studio: TheBloke/CodeLlama-13B-Instruct-GGUF
#   - LocalAI: your-model-name
EXPO_PUBLIC_LOCAL_AI_MODEL=codellama:13b

# AI Server Type (optional)
# Options: ollama, openai, custom
# Default: ollama
EXPO_PUBLIC_LOCAL_AI_TYPE=ollama

# API Key (optional, if your server requires authentication)
# EXPO_PUBLIC_LOCAL_AI_API_KEY=your-api-key

# Request timeout in milliseconds (optional)
# Default: 30000 (30 seconds)
# EXPO_PUBLIC_LOCAL_AI_TIMEOUT=30000

# Maximum tokens to generate (optional)
# Default: 2048
# EXPO_PUBLIC_LOCAL_AI_MAX_TOKENS=2048

# Temperature for generation (optional)
# Range: 0.0 (deterministic) to 1.0 (creative)
# Default: 0.7
# EXPO_PUBLIC_LOCAL_AI_TEMPERATURE=0.7

# ============================================================================
# Local Voice Configuration (Future Feature)
# ============================================================================

# Local Voice Server URL (for future voice integration)
# EXPO_PUBLIC_LOCAL_VOICE_URL=http://localhost:5002

# Voice model to use
# Examples: whisper-large, coqui-tts, piper
# EXPO_PUBLIC_LOCAL_VOICE_MODEL=whisper-large

# ============================================================================
# Server Configuration
# ============================================================================

# Backend server URL (if using self-hosted backend)
# Leave empty to use default cloud backend
# EXPO_PUBLIC_HAPPY_SERVER_URL=

# ============================================================================
# Debug Settings
# ============================================================================

# Enable debug mode (shows detailed logs)
# EXPO_PUBLIC_DEBUG=0

# Enable Local AI debug logging
# EXPO_PUBLIC_LOCAL_AI_DEBUG=0

# Log to server for AI debugging (use carefully, may expose data)
# PUBLIC_EXPO_DANGEROUSLY_LOG_TO_SERVER_FOR_AI_AUTO_DEBUGGING=0

# ============================================================================
# Advanced Configuration
# ============================================================================

# Context window size in tokens (optional)
# EXPO_PUBLIC_LOCAL_AI_CONTEXT_SIZE=4096

# Number of conversation messages to keep in context (optional)
# EXPO_PUBLIC_LOCAL_AI_MAX_HISTORY=10

# Nucleus sampling parameter (optional)
# EXPO_PUBLIC_LOCAL_AI_TOP_P=0.95

# Top-k sampling parameter (optional)
# EXPO_PUBLIC_LOCAL_AI_TOP_K=40

# Repetition penalty (optional)
# EXPO_PUBLIC_LOCAL_AI_REPEAT_PENALTY=1.1

# ============================================================================
# Analytics (Optional)
# ============================================================================

# PostHog API Key (for analytics, leave empty to disable)
# EXPO_PUBLIC_POSTHOG_API_KEY=

# Revenue Cat Keys (for in-app purchases, leave empty if not using)
# EXPO_PUBLIC_REVENUE_CAT_APPLE=
# EXPO_PUBLIC_REVENUE_CAT_GOOGLE=
# EXPO_PUBLIC_REVENUE_CAT_STRIPE=

# ============================================================================
# Notes
# ============================================================================

# 1. Never commit .env to version control
# 2. Restart the dev server after changing environment variables
# 3. For mobile access, use your computer's local IP instead of localhost
# 4. Ensure firewall allows connections to your local AI server
# 5. See docs/LOCAL_AI_INTEGRATION.md for detailed setup instructions
